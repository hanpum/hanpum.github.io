<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"hanpum.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="hanpum">
<meta property="og:url" content="https://hanpum.github.io/index.html">
<meta property="og:site_name" content="hanpum">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="hanpu.mwx">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://hanpum.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>hanpum</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">hanpum</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/hanpum" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hanpum.github.io/2020/04/30/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/ai.jpeg">
      <meta itemprop="name" content="hanpu.mwx">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hanpum">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/30/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-30 11:41:44" itemprop="dateCreated datePublished" datetime="2020-04-30T11:41:44+08:00">2020-04-30</time>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>357</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hanpum.github.io/2020/04/30/nlp/word2vec/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/ai.jpeg">
      <meta itemprop="name" content="hanpu.mwx">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hanpum">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/30/nlp/word2vec/" class="post-title-link" itemprop="url">词向量: word2vec</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-30 00:00:00" itemprop="dateCreated datePublished" datetime="2020-04-30T00:00:00+08:00">2020-04-30</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NOTES/" itemprop="url" rel="index"><span itemprop="name">NOTES</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>18k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>16 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <div id="content">

<div id="outline-container-orgd58f374" class="outline-2">
<h2 id="orgd58f374">概述</h2>
<div class="outline-text-2" id="text-orgd58f374">
</div>
<div id="outline-container-org574db9d" class="outline-3">
<h3 id="org574db9d">词向量</h3>
<div class="outline-text-3" id="text-org574db9d">
<p>
<code>word vector</code>, 从字面意思来理解，就是词向量。也就是使用一个向量来表示一个词, 其好处就是能够使用数
学语言来描述文字符号，从而可以使用数学工具对齐进行分析。
</p>

\begin{equation*}
  \mathbf{u} = [x_{0}, x_{1}, \cdots, x_{d}]^T
\end{equation*}

<p>
从发展历史来看，词向量大致可以分为如下几种:
</p>
<ol class="org-ol">
<li>特征工程</li>
<li>共现矩阵</li>
<li>predication-based</li>
</ol>

<p>
下面分别对其进行简单介绍
</p>
</div>
</div>

<div id="outline-container-org8233170" class="outline-3">
<h3 id="org8233170">特征向量</h3>
<div class="outline-text-3" id="text-org8233170">
<p>
特征向量属于特征工程时代的产物，就是人工设计一组特征(一般是离散的), 然后提取对应的特征值得到一组
向量。比如，假设有如下三个特征维度:
</p>
<ol class="org-ol">
<li>单词长度</li>
<li>是否是名词: 可能取值 0,1</li>
<li>是否是人称代词：可能取值 0,1</li>
</ol>

<p>
那么，可以将所有word表示为一个三维的向量 \(X\), 
</p>
<ul class="org-ul">
<li>\(X[0]\) : 记录这个单词的长度</li>
<li>\(X[1]\) : 记录这个单词是不是名词</li>
<li>\(X[2]\) : 记录这个单词是不是人称代词</li>
</ul>

<p>
相应的就可以计算得到每个单词对应的向量
</p>

<pre class="example">
我们 -> [2, 1, 1]
热爱 -> [2,0,0]
NLP -> [3,1,0]</pre>

<p>
这种表示方法比较简单，是一个离散的特征表示，表达能力有限，而且需要人工设计所有的特征，系统性能的
好坏极度的依赖特征的选择。
</p>
</div>
</div>

<div id="outline-container-orgb0bd789" class="outline-3">
<h3 id="orgb0bd789">共现矩阵</h3>
<div class="outline-text-3" id="text-orgb0bd789">
<p>
共现矩阵的依据如下:
</p>

<div class="notes">
<p>
You shall know a word by the company it keeps. 
		     – (<a href="https://en.wikipedia.org/wiki/John_Rupert_Firth" target="_blank" rel="noopener">Firth, J. R. 1957:11</a>)
</p>

</div>

<p>
通俗地讲，就是 <b>从其交友知其为人</b>, 也就是一个单词的含义可以从它周围的单词推测出来。这个道理也很
好懂。在遣词造句的时候，我们会不自知的遵循某些知识，这些知识可以是语言学知识，比如主谓宾结构;可
以是常理知识，比如 <b>X的首都是Y</b> ，X会是个国家，Y会是个城市等等。
</p>

<p>
共现矩阵的做法就是统计两个词同时出现的次数，用某一个词旁边所有其它词出现的次数来作为它的词向量。
比如，给定下面两句话: 
</p>

<pre class="example">
all that glitters is not gold 
all is well that ends well</pre>

<p>
如果两个单词 \(w_i, w_j\) 相邻(也可以放宽条件，比如相隔不超过N个单词)，将对应的计数 \(cnt_{i,j}\) 加
一，于是可以得到如下矩阵:
</p>

<table id="org8fc0316" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col class="org-left">

<col class="org-right">

<col class="org-right">

<col class="org-right">

<col class="org-right">

<col class="org-right">

<col class="org-right">

<col class="org-right">

<col class="org-right">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left"> </th>
<th scope="col" class="org-right">all</th>
<th scope="col" class="org-right">that</th>
<th scope="col" class="org-right">glitters</th>
<th scope="col" class="org-right">is</th>
<th scope="col" class="org-right">not</th>
<th scope="col" class="org-right">gold</th>
<th scope="col" class="org-right">well</th>
<th scope="col" class="org-right">ends</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">all</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-left">that</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-left">glitters</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-left">is</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-left">not</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-left">gold</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-left">well</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-left">ends</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
</tbody>
</table>

<p>
这里的每一行或者每一列即可用来表示一个单词的词向量，该向量的长度为词表大小 \(\parallel V \parallel\) 。
在实际情况中, \(\parallel V \parallel\) 可以达到上百万, 所以这里得到的是一个高维稀疏
矩阵，冗余度比较大，直接使用效果不佳，一般对其进行降维压缩处理, 比如使用SVD(奇异值分解), 将其从
\(\mathbb{R}^{V \times V}\) 降维成 \(\mathbb{R}^{V \times K}\) , 一般 \(K \ll V\), 比如取300，500，1000等。
</p>

<p>
这个方法直接使用的效果并不是很好，因为有很多常见的词，比如 <code>a</code>, <code>the</code>, <code>is</code> ， 这些词出现的概率
一般都比较高，因此很有可能出现如下结果: 
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col class="org-left">

<col class="org-left">

<col class="org-left">

<col class="org-left">

<col class="org-left">

<col class="org-left">

<col class="org-left">

<col class="org-left">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left"> </th>
<th scope="col" class="org-left">\(\cdots\)</th>
<th scope="col" class="org-left">that</th>
<th scope="col" class="org-left">is</th>
<th scope="col" class="org-left">\(\cdots\)</th>
<th scope="col" class="org-left">gold</th>
<th scope="col" class="org-left">well</th>
<th scope="col" class="org-left">\(\cdots\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(\cdots\)</td>
<td class="org-left">\(\cdots\)</td>
<td class="org-left">\(\cdots\)</td>
<td class="org-left">\(\cdots\)</td>
<td class="org-left">\(\cdots\)</td>
<td class="org-left">\(\cdots\)</td>
<td class="org-left">\(\cdots\)</td>
<td class="org-left">\(\cdots\)</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">glitters</td>
<td class="org-left">\(\cdots\)</td>
<td class="org-left">90000</td>
<td class="org-left">100000</td>
<td class="org-left">\(\cdots\)</td>
<td class="org-left">30</td>
<td class="org-left">50</td>
<td class="org-left">\(\cdots\)</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">\(\cdots\)</td>
<td class="org-left">\(\cdots\)</td>
<td class="org-left">\(\cdots\)</td>
<td class="org-left">\(\cdots\)</td>
<td class="org-left">\(\cdots\)</td>
<td class="org-left">\(\cdots\)</td>
<td class="org-left">\(\cdots\)</td>
<td class="org-left">\(\cdots\)</td>
</tr>
</tbody>
</table>

<p>
但是对于两个不同的单词，比如 <code>gold</code> ， <code>gitters</code> ， <code>gold is</code> ， <code>gitters is</code> 出现的次数是差不多
的，也就是说 <code>is</code> 对区分 <code>gold</code> ， <code>gitters</code> 并没有多大帮助。从而导致词向量中被一些无意义的分量主导，
而真正有价值的信息被湮没了。
</p>

<p>
针对这种情况，可以对共现频次统计量做一些高频惩罚，比如，取平方根，取对数等, 取上限等。 
</p>

\begin{equation*}
  cnt_{i,j}^{'} = \sqrt{cnt_{i,j}}
  \qquad or \qquad cnt_{i,j}^{'} = log(cnt_{i,j}+1)
  \qquad or \qquad cnt_{i,j}^{'} = min(cnt_{i,j}, \mathbf{C})
\end{equation*} 

<p>
<a class="org-ref-reference" href="#pennington-2014-glove">pennington-2014-glove</a> 给出的结果验证了上述问题。
</p>


<div class="figure">
<p><img src="gloveSVD.jpg" width="400px">
</p>
</div>

<p>
其中 <b>SVD-L</b> 表示取 \(log\) ， <b>SVD-S</b> 表示取平方根。
</p>
</div>
</div>

<div id="outline-container-org0973953" class="outline-3">
<h3 id="org0973953">任务导向</h3>
<div class="outline-text-3" id="text-org0973953">
<p>
基于任务导向的词向量学习方法和上面的共现矩阵一样，其最基础的依据也是根据单词的上下文来推测其含义。
不过，实现的方式和共现矩阵却大相径庭，其策略是以终为始, 设计一个或者一组任务，将每个单词或者字映
射为一个向量作为输入, 然后通过机器学习的方式对设计的任务进行优化，优化过程中也会对词向量进行优化。
当任务训练完成之后，将优化完毕的词向量取出即为最终每个单词的向量表示。
</p>

<p>
本文要讲的word2vec (<a class="org-ref-reference" href="#mikolov-2013-effic-estim">mikolov-2013-effic-estim</a>,<a class="org-ref-reference" href="#mikolov-2013-distr-repres">mikolov-2013-distr-repres</a>), 
glove (<a class="org-ref-reference" href="#pennington-2014-glove">pennington-2014-glove</a>), 以及 Bengio 03年的 NNLM (<a class="org-ref-reference" href="#bengio-2003-neural-probab">bengio-2003-neural-probab</a>), 
以及后面一系列的预训练模型直到集大成的 BERT (<a class="org-ref-reference" href="#devlin-2018-bert">devlin-2018-bert</a>) 都属于此列。
</p>

<p>
接下来对 word2vec 做一些深入分析, glove 留待下一篇文章来分析了。
</p>
</div>
</div>
</div>

<div id="outline-container-org2a92ec5" class="outline-2">
<h2 id="org2a92ec5">设计思路</h2>
<div class="outline-text-2" id="text-org2a92ec5">
<p>
word2vec 于 13 年在 <a class="org-ref-reference" href="#mikolov-2013-effic-estim">mikolov-2013-effic-estim</a>,<a class="org-ref-reference" href="#mikolov-2013-distr-repres">mikolov-2013-distr-repres</a> 这两篇 paper 中提出来
的。其主要设计思路如上面所说，就是使用单词周围的单词来预测给定单词的概率，依实现方式可以分为
<b>SKIP_GRAM</b> 和 <b>CBOW(Continues Bag of Words)</b> 两种，下面一一道来。
</p>
</div>
</div>

<div id="outline-container-orga843fe7" class="outline-2">
<h2 id="orga843fe7">skip-gram</h2>
<div class="outline-text-2" id="text-orga843fe7">
</div>
<div id="outline-container-org352a657" class="outline-3">
<h3 id="org352a657">目标函数</h3>
<div class="outline-text-3" id="text-org352a657">
<p>
给定一个句子 \(S = [w_0, w_1, \cdots, w_T]\), 以及句子中的一个位置 \(t, t \in [0, T]\) , \(S_t\) 表示位
置 \(t\) 处的单词，skip-gram的目标是 通过 \(S_t\) 来预测其周围单词出现的概率。
</p>


<div class="figure">
<p><img src="skipGram.png">
</p>
</div>

<p>
一般来说，两个单词距离越远，其相关性越弱，因此为了简化计算，可以将计算范围限定在一个窗口区域内
\(2M+1\) , 上图画出了 \(M=2\) 的情况。 假设这 \(2M\) 个概率事件相互独立，那么可以通过简单的相乘来计算这
一组事件的概率:
</p>

\begin{equation*}
  \mathcal{L}(t) = \prod\limits_{\substack{i\neq 0 \\ i=-M}}^{M} p(S_{t+i}=w_{t+i}|S_t=w_{t}, \Theta)
\end{equation*}			     

<p>
为了书写方便， 将 \(p(S_{t+i}=w_{t+i}|S_{t}=w_t, \Theta)\) 简写为 \(p(w_{t+i}|w_t, \Theta)\)
</p>

\begin{equation} \label{eq-skip-gram-word}
  \mathcal{L}(t) = \prod\limits_{\substack{i\neq 0 \\ i=-M}}^{M} p(w_{t+i}|w_{t}, \Theta)
\end{equation}			     

<p>
这样就给出了基于单词 \(S_t\) 预测其上下文的一个评价标准。式 \eqref{eq-skip-gram-word} 只是针对句子中
的一个位置 <code>t</code> ， 对于整个句子 <code>S</code> 中每一个位置(不包括开头和结尾的 <code>M</code> 个)，都可以计算得
到这样一个概率，假设不同的 <code>t</code> 对应的概率事件相互独立，那么就可以计算出整个句子上的概率来:
</p>

     \begin{equation} \label{eq-skip-gram-sentense}
\mathcal{L}(S) = \prod\limits_{t=M}^{T-M} \prod\limits_{\substack{i\neq 0 \\ i=-M}}^{M} p(w_{t+i}|w_{t}, \Theta)
     \end{equation}	

<p>
接下来对 \eqref{eq-skip-gram-sentense} 做如下的改造:
</p>
<ol class="org-ol">
<li>取 <code>log</code>, 计算方便，而且避免数值计算的精度丢失问题</li>
<li>除以 <code>T</code> , 移除句子长度的影响</li>
<li>取负，将最大化问题转为最小化问题</li>
</ol>

<p>
即可得到 skip-gram 模型最终的 <code>loss</code> 函数
</p>

     \begin{equation} \label{eq-skip-gram-loss}
\mathcal{J}(\Theta) = -\frac{1}{T}\sum\limits_{t=M}^{T-M} \sum\limits_{\substack{i\neq 0 \\ i=-M}}^{M} log\ p(w_{t+i}|w_{t},\Theta) 
     \end{equation}	

<p>
\(\Theta\) 是模型参数，随后会有详细解释。 接下来的问题就是，如何计算 \(p(w_{t+i}|w_t, \Theta)\) 这个
条件概率呢?
</p>
</div>
</div>

<div id="outline-container-org683bb72" class="outline-3">
<h3 id="org683bb72">条件概率</h3>
<div class="outline-text-3" id="text-org683bb72">
<p>
为了描述方面，上面 \(w_t, w_{t+i}\) 表示的是一个具体的单词，由于每个单词都对应一个向量，所以 \(\mathbf{w_t},
   \mathbf{w_{t+i}}\) 也可以等价为对应的向量，以后都按照这种方式来表示。
</p>

     \begin{equation*}
\mathbf{W} = \left[
\begin{array}{cccc}
  w_{00} & w_{01} & \cdots & w_{0D} \\
  w_{10} & w_{11} & \cdots & w_{1D} \\
  \vdots & \vdots & \ddots & \vdots \\
  w_{V0} & w_{V1} & \cdots & w_{VD} 
\end{array} \right] 
= \left[
\begin{array}{c}
  \mathbf{w_{0}}^T \\
  \mathbf{w_{1}}^T \\
  \vdots \\
  \mathbf{w_{V}}^T
\end{array} \right]
     \end{equation*}


<p>
这个时候 \(p(\mathbf{w_{t+i}}|\mathbf{w_{j}}, \Theta=\{W\})\) ，计算的是给定一个向量，另一个向量出现的概率，很自
然的，可以计算这两个向量的相似度，然后做一个归一化即可得到概率。向量的相似度计算方式有很多，
<code>word2vec</code> 中采用的是 <a href="https://baike.baidu.com/item/%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6/17509249" target="_blank" rel="noopener">余弦相似度</a>，也即: 
</p>

   \begin{equation*}
     cos(\mathbf{w}_{i}, \mathbf{w}_{j}) = \frac
     {
\mathbf{w}_{i}\cdot \mathbf{w}_{j}
     }
     {
\parallel \mathbf{w}_{i} \parallel
\cdot
\parallel \mathbf{w}_{j} \parallel
     }
   \end{equation*}

<p>
这个公式可以由欧几里得点积公式
</p>

\begin{equation*}
  [x_1, y_1] \cdot [x_2, y_2] = x_1 \cdot x_2 + y_1 \cdot y_2
\end{equation*}

<p>
和余弦定理
</p>

\begin{equation*}
  \mathbf{c}^2 = \mathbf{a}^2 + \mathbf{b}^2 - 2\parallel \mathbf{a} \parallel \cdot \parallel \mathbf{b} \parallel \cdot \ cos(\theta), 
  \qquad \text{in which } \mathbf{c} = \mathbf{a} + \mathbf{b}
\end{equation*}

<p>
推导得到（将向量表示代入余弦定理，结合点积公式整理化简），具体细节这里不做展开。
</p>

<p>
在实际使用中，$\color{red}{如果假设 \mathbf{w}_i, \mathbf{w}_j 都是单位向量}(待确认)$，那么，余弦相似度中的分母可
以忽略。于是可以得到未归一化的概率:
</p>

\begin{equation*} 
  \hat{p}(\mathbf{w}_{i}|\mathbf{w}_{j}) = \mathbf{w}_{i} \cdot \mathbf{w}_{j}
\end{equation*}

<p>
使用 <code>softmax</code> 函数进行归一化，即可得到最终的条件概率:
</p>

\begin{equation} \label{eq-softmax}
  p(\mathbf{w}_{i}|\mathbf{w}_{j}) \ =\ \mathcal{S}(\mathbf{w_i} \cdot \mathbf{w_j}) \ =\ \frac{exp(\mathbf{w}_{i} \cdot \mathbf{w}_{j})}
				   {\sum\limits_{j=0}^{V} exp(\mathbf{w}_{j} \cdot \mathbf{w}_{j})}
\end{equation}
</div>
</div>

<div id="outline-container-org591c30f" class="outline-3">
<h3 id="org591c30f">优化公式</h3>
<div class="outline-text-3" id="text-org591c30f">
<p>
给出了 <code>loss</code> 函数之后，使用梯度下降算法进行优化即可, 先计算梯度
</p>

\begin{equation} \label{eq-skip-gram-gradient}
  \begin{array}{ccc}
    \frac{\partial \mathcal{J}}{\partial \mathbf{w_{i}}} & = &
					      -\frac{1}{T}
					      \sum\limits_{t=M}^{T-M} \sum\limits_{\substack{i\neq 0 \\ i=-M}}^{M}
    \frac{\partial log\ p(\mathbf{w_{t+i}}|\mathbf{w_{t}})}{\partial \mathbf{w_{i}}}
  \end{array}
\end{equation}

<p>
然后利用梯度下降更新参数即可
</p>

\begin{equation} \label{eq:gradient-descent}
  \mathbf{\hat{w}_i} = \mathbf{w_i} - \eta \cdot \frac{\partial \mathcal{J}}{\partial \mathbf{w_i}}
\end{equation}

<p>
其中 \(\eta\) 为学习率。
</p>

<p>
<code>softmax</code> 有一个很大的好处就是求导简单，因为:
</p>

\begin{equation*}
      \begin{array}{rcl}
	\mathcal{S}(x_{i}) & = & \frac{e^{y_i}}{ \sum_{j} e^{y_j}}, \qquad \textrm{in which } y_i = f(x_i) \\
	\\
	\frac{\partial \mathcal{S}} {\partial x_{i}}
		 % & = & \frac{
		 %       e^{y_i}
		 %       \cdot
		 %       \frac{\partial {y_{i}}}{\partial x_{i}}
		 %       \cdot
		 %       \big(\sum_{j}e^{y_j}\big)
		 %       \quad - \quad
		 %       e^{y_{i}}
		 %       \cdot
		 %       e^{y_i}
		 %       \cdot
		 %       \frac{\partial {y_{i}}}{ \partial x_{i}}
		 %       } {
		 %       \big(\sum_{j} e^{y_{j}}\big)^{2}
		 %       } \\
		 % & = & [\mathcal{S}(x_{i}) - \mathcal{S}(x_{i})^{2}]
		 %       \cdot 
		 %       \frac{\partial {y_{i}}}{ \partial x_{i}} \\
		 & = & \mathcal{S}(x_{i} ) \cdot [1 - \mathcal{S}(x_{i})]
		       \cdot 
		       \frac{\partial {y_{i}}}{ \partial x_{i}} \\
	\\ 
	\frac{\partial log \mathcal{S}} {\partial x_{i}}
		 & = & [1 - \mathcal{S}(x_{i})]
		       \cdot 
		       \frac{\partial {y_{i}}}{ \partial x_{i}} 
      \end{array}
    \end{equation*}

<p>
也就是知道 \(\mathcal{S}(x_i)\) 之后, 做简单加减运算就可以得到 \(log\ \mathcal{S}(x_i)\) 的导数。
</p>

<p>
不过，当 \(y = \mathbf{w}_i \cdot \mathbf{w}_j\) 的时候, 有
</p>

\begin{equation*}
  \frac{\partial y } {\partial \mathbf{w}_{j}} = \left\{
      \begin{array}{cl}
	\mathbf{w}_{i}, && i \neq j \\
	2 \cdot \mathbf{w}_{i}, && i = j
      \end{array}
  \right.
\end{equation*}

<p>
也即计算梯度的时候需要做一个 <code>if</code> 判断, 做批量并行处理的时候, 为了实现这个 <code>if</code> 
需要花点小心思。当然也可以偷懒绕过这个问题，最简单的就是引入另一个参数 \(\mathbf{Q} \in \mathbb{R}^{V \times D}\) , 其大小和含义和
\(\mathbf{W}\) 一样，每一行都对应一个单词的词向量，不过 \(\mathbf{W}\) 用于表示中间那个单词，而
\(\mathbf{Q}\) 用于表示需要被预测的单词，此时，\(y = \mathbf{q}_i \cdot \mathbf{w}_j\) , 
</p>

\begin{equation*}
  \left \{
    \begin{array}{rcl}
      \frac{\partial y } {\partial \mathbf{q}_{j}} & = & \mathbf{w}_{i}\\
      \frac{\partial y } {\partial \mathbf{w}_{i}} & = & \mathbf{q}_{j}\\
    \end{array}
  \right.
\end{equation*}

<p>
这样一来, 求导的时候 \(\mathbf{q}_i,\mathbf{u}_n\) 互不干扰，从而避免了上述问题。 代入式
\eqref{eq-skip-gram-gradient}  中可以得到
</p>

\begin{equation*}
  \begin{array}{rcl}
    \frac{\partial \mathcal{J}}{\partial \mathbf{w_{i}}} & = & -\frac{1}{T}
					      \sum\limits_{t=M}^{T-M} \sum\limits_{\substack{i\neq 0 \\ i=-M}}^{M}
    \left[1 - \mathcal{S}(\mathbf{q_i} \cdot \mathbf{w_j})\right] \cdot \mathbf{q_j}
    \\
    \\
    \frac{\partial \mathcal{J}}{\partial \mathbf{q_{j}}} & = & -\frac{1}{T}
					      \sum\limits_{t=M}^{T-M} \sum\limits_{\substack{i\neq 0 \\ i=-M}}^{M}
    \left[1 - \mathcal{S}(\mathbf{q_i} \cdot \mathbf{w_j})\right] \cdot \mathbf{w_i}
  \end{array}
\end{equation*}

<p>
但是这样一来的话，模型参数量翻了一倍，而且每个单词会对应两个向量 \(\mathbf{q}_i, \mathbf{w}_i\) ，
cs224n 中给出的答案是简单的将其做一个平均得到最终的向量。
</p>

<p>
至此， <code>skip-gram</code> 的模型架构以及更新细节就讲解完毕了。在后面的具体实现部分，我们会进一步分析
<code>word2vec</code> 源码实现中的一些优化，到时候可以看到实际的loss函数和这里讲的是有一些差异的。
</p>
</div>
</div>
</div>

<div id="outline-container-orgf103723" class="outline-2">
<h2 id="orgf103723">CBOW</h2>
<div class="outline-text-2" id="text-orgf103723">
</div>
<div id="outline-container-org22ea5ef" class="outline-3">
<h3 id="org22ea5ef">目标函数</h3>
<div class="outline-text-3" id="text-org22ea5ef">
<p>
和 <code>skip-gram</code> 相比， <code>cbow</code> 的模型架构更接近传统意义的神经网络。
</p>


<div class="figure">
<p><img src="cbow.png">
</p>
</div>

<p>
左图是传统 <code>RNN</code> 的一个简化示意图，输入 \(\mathbf{x_t}\) 向量 (这里可以看做是词向量) 一般会经过一个非线性变化, 比如
\(\sigma(\mathbf{w} \cdot \mathbf{x} + b)\) , 得到中间隐层表示 \(\mathbf{h_t}\) , 然后基于
\(\mathbf{h_t}\) 和目标向量 \(\mathbf{w_t}\) 计算 <code>loss</code> 函数。
</p>

<p>
从上图可以看出来， <code>CBOW</code> (上面的 <code>skip-gram</code> 也是如此) 直接移除了中间的隐层计算。这是因为二者的
关注点不一样。在左图的网络中， 主要任务是训练整个网络优化最后的输出，比如
<a class="org-ref-reference" href="#bengio-2003-neural-probab">bengio-2003-neural-probab</a> 中主要是为了训练得到完整的网络用作语言模型，隐层中 <code>RNN</code> 的
引入是为了更好地解决语言模型的依赖问题, 最终得到词向量只是附带的一个产物。而在 <code>word2vec</code> 中的首
要任务就是得到词向量, 因此可以做此简化。
</p>

<p>
从上面的图可以看出来， <code>CBOW</code> 的目标函数也是比较简单的, 下面直接给出:
</p>

\begin{equation} \label{eq-cbow-loss}
  \begin{array}{rcl}
    h_{t} & = & \sum\limits_{\substack{i \neq 0\\ i=-M}}^{M} w_{t+i} \\
    \\
    \mathcal{J}(\Theta) & = & -\frac{1}{T}\sum\limits_{t=M}^{T-M} log\ p(w_{t}|h_{t},\Theta) 
  \end{array}
\end{equation}

<p>
整个和 <code>skip-gram</code> 非常像，这里就不做展开了。
</p>
</div>
</div>
</div>

<div id="outline-container-org9cf9373" class="outline-2">
<h2 id="org9cf9373">具体实现</h2>
<div class="outline-text-2" id="text-org9cf9373">
</div>
<div id="outline-container-orgb28792c" class="outline-3">
<h3 id="orgb28792c">overview</h3>
<div class="outline-text-3" id="text-orgb28792c">
<p>
本节讲一下 <a class="org-ref-reference" href="#mikolov-2013-distr-repres">mikolov-2013-distr-repres</a> 中提到的一些优化技巧。
</p>

<p>
在式 \eqref{eq-skip-gram-loss} \eqref{eq-cbow-loss} 中, 都需要计算两个向量之间的一个相似度，
\(p(\mathbf{w_i}|\mathbf{w_j})\) , 这里回顾一下计算公式 \eqref{eq-softmax}
</p>

\begin{equation*} 
  p(\mathbf{w}_{i}|\mathbf{w}_{j}) \ =\ \mathcal{S}(\mathbf{w_i} \cdot \mathbf{w_j}) \ =\ \frac{exp(\mathbf{w}_{i} \cdot \mathbf{w}_{j})}
					  {\sum\limits_{j=0}^{V} exp(\mathbf{w}_{j} \cdot \mathbf{w}_{j})}
\end{equation*}

<p>
其分母需要在整个词表大小 \(V\) 上计算, 前面说过， \(\parallel V \parallel\) 在实际使用可能是上百万的
量级，计算量比较大，计算示意图如下。
</p>


<div class="figure">
<p><img src="softmax.png">
</p>
</div>

<p>
接下来讲到的几个优化技巧中，有两个就是针对这一项进行展开。
</p>
</div>
</div>

<div id="outline-container-org1098d9e" class="outline-3">
<h3 id="org1098d9e">hierarchical softmax</h3>
<div class="outline-text-3" id="text-org1098d9e">
<p>
原始softmax的计算过程是一个扁平决策的过程，而 <code>hierarchical softmax</code> 使用的是决策树。
</p>

<p>
其计算模型如下图所示
</p>


<div class="figure">
<p><img src="hierarchical-softmax.png">
</p>
</div>

<p>
词表中的每个单词都对应决策树中的一个叶子节点(图中的方框), 每个中间节点(图中的圆圈) 对应有一个向
量 \(\mathbf{u}\) , 将输入 \(\mathbf{w}_t\) 和 \(\mathbf{u}\) 点积之后，输入 <code>sigmoid</code> 做一个二分类，
如果小于0.5， 走左子树，否则走右子树。重复上述过程，直到抵达叶子节点。 
</p>

<p>
在训练过程中，由于目标单词 \(w_i\) 是已知的，那么从根节点到 \(w_i\) 的路径(图中的加粗路
径)就是已知的，对于任何一个输入, 如果构造的树是完全二叉树，则只需要计算 \(log_2\ V\) 次即可， 而不
是原始 softmax 中的 \(V\) 次。关于这棵二叉树的构造下一节会做进一步展开。
</p>

<p>
由于 <code>hierarchical softmax</code> 修改了之前的 <code>loss</code> 计算，因此参数更新方式也要做相应调整。
</p>

<p>
假设目标单词 \(w_i\) 对应的路径为 \(\pi_{i} = \{n_0, n_1, \cdots, n_M\}\) , 对应的每个节点的的标签
(走左子树还是右子树)为 \(L_i = \{l_0, l_1, \cdots, l_M\}, l_i \in \{0,1\}\) , 那么，对于路径上的每
一个节点 \(n_i\) , 有 
</p>

\begin{equation} \label{eq-sigmoid}
  \begin{array}{rcl}
  p(n_i|w_t) & = & \Bigg\{\begin{array}{cr}
			 \frac{1}{1 + e^{-u_{i} \cdot w_{t}}}, & \quad l_{i} = 1 \\
			 \\
			 1 - \frac{1}{1 + e^{-u_{i} \cdot w_{t}}}, & \quad l_{i} = 0
		       \end{array} \\
	     \\    
	     & = & \left( \frac{1}{1 + e^{-u_{i} \cdot w_{t}}} \right)^{l_{i}}
		   \left( 1 - \frac{1}{1 + e^{-u_{i} \cdot w_{t}}} \right)^{1-l_{i}} \\
    \\
	     & = & \sigma(u_{j} \cdot w_{t})^{l_{i}}
		   \times
		   \left(1 - \sigma(u_{j} \cdot w_{t})\right)^{1-l_{i}}
    \end{array}
\end{equation}

<p>
据此可以得到对应的 <code>loss</code> 函数:
</p>
\begin{equation} \label{eq-hsm-loss}
  \begin{array}{rcl}
    p(w_{i}|w_{t}) & = & \prod\limits_{j=0}^{M} p(n_{j}|w_{t}) \\
    \\
    \mathcal{L(w_{i})} & = & -log\ p(w_{i}|w_{t}) \\
		   & = & -\sum\limits_{j=0}^{M}
			 l_{j} \cdot log\ \sigma(u_{j} \cdot w_{t})
			 +
			 (1-l_{j}) \cdot log\ \left[1 - \sigma(u_{j} \cdot w_{t}) \right] \\
  \end{array}
\end{equation}

<p>
结合 <code>sigmoid</code> 函数的梯度计算
</p>

\begin{equation*} 
  \begin{array}{rclcl}
    \sigma^{'}(x) & = & \sigma(x) \cdot \left( 1 - \sigma(x) \right) \\
    \\
    \partial log \sigma(x) / \partial x & = & 1- \sigma(x) \\
    \\
    \partial log\ \left( 1-\sigma(x) \right) / \partial x  & = & \frac{1}{1-\sigma(x)}
								      \cdot -1 \cdot
								      \sigma(x) \cdot \left[ 1 - \sigma(x) \right] & = & -\sigma(x) \\
  \end{array}
\end{equation*}

<p>
可以得到对应的梯度
</p>

\begin{equation*}
  \begin{array}{rclcl}
    \frac{\partial \mathcal{L}(w_{i})}{ \partial w_{i}} & = & -\sum\limits_{j=0}^{M}
						       l_{j} \cdot \left[ 1- \sigma(u_{j} \cdot w_{t}) \right]
						       +
							      (1-l_{j}) \cdot - \sigma(u_{j} \cdot w_{t}) 
							& = & -\sum\limits_{j=0}^{M}
							      \left[
							      l_{i} - \sigma(u_{j} \cdot w_{t}) 
							      \right] \cdot u_{j} \\
    \\
    \frac{\partial \mathcal{L}(w_{i})}{ \partial u_{j}} & = & -\left[
							      l_{j} - \sigma(u_{j} \cdot w_{t}) 
							      \right] \cdot w_{t}
  \end{array}
\end{equation*}

<p>
<code>word2vec</code> 源码中使用的更新公式和上面有一些差异，主要是因为式 <a href="#eq-sigmoid">eq-sigmoid</a> 中的 <code>label</code> 和
<code>word2vec</code> 的恰好反过来，如果要保持一致的话，使用 \(1-l_j\) 替换上述式子中的 \(l_j\) , 即可得到和
<code>word2vec</code> 源码中一样的结果。
</p>
</div>
</div>

<div id="outline-container-org58391ab" class="outline-3">
<h3 id="org58391ab">Huffman编码</h3>
<div class="outline-text-3" id="text-org58391ab">
<p>
上一节在分析 <code>hierarchical softmax</code> 的时候，使用的决策树是一颗完全二叉树，也就是所有单词无差别对
待，全部分布在树的最后一层或倒数第二层的右边，如果将每个单词出现的概率作为其权重，那么可以得到一
个 <a href="https://baike.baidu.com/item/%E5%93%88%E5%A4%AB%E6%9B%BC%E6%A0%91" target="_blank" rel="noopener">带权二叉树</a>，其带权路径长度为 \(log V\) 。这棵树可以进一步优化得到带权路径最小的 <code>Huffman</code> 树，
也成为最优二叉树。
</p>

<p>
关于带权二叉树的细节这里不做展开，细节可以参考 <a href="https://baike.baidu.com/item/%E5%93%88%E5%A4%AB%E6%9B%BC%E6%A0%91" target="_blank" rel="noopener">百度百科</a> 。其构建方式如下:
</p>

<ul class="org-ul">
<li>将所有单词作为一棵树，组成森林 \(\mathcal{F} = \{t_0, t_1, \cdots, t_V\}\)</li>
<li>从 \(\mathcal{F}\) 中挑选权重最小的两棵树 \(t_i, t_j\), 将其从 \(\mathcal{F}\) 中移除</li>
<li>新建一棵树 \(\hat{t}\) , 其左右子树分别为上面挑选出来的为 \(t_i, t_j\)</li>
<li>将 \(\hat{t}\) 添加到 \(\mathcal{F}\) 中</li>
<li>重复上面三步，直到 \(\mathcal{F}\) 中只剩下一棵树，即为最终的 <code>Huffman</code> 树。</li>
</ul>

<p>
完成 <code>Huffman</code> 树构建之后，其他的计算过程和上一节的完全一样。
</p>
</div>
</div>

<div id="outline-container-org0ca17be" class="outline-3">
<h3 id="org0ca17be">交叉熵</h3>
<div class="outline-text-3" id="text-org0ca17be">
<p>
为了分析接下来的负采样技术，我们先换一个角度来看 <code>loss</code> 函数, 以 <code>CBOW</code> 为例。
</p>

\begin{equation*}
  \mathcal{J}(w_{t}, \Theta) = -\frac{1}{T} \sum\limits_{t=M}^{t=T-M} log\ p(w_{t}|h_{t})
\end{equation*}

<p>
也就是使用某种方式计算出目标单词的概率 \(p(w_t|\cdot)\) , 然后取 <code>log</code> , 最后在整个语料上取一个平
均。 <code>skip-gram</code> 的 <code>loss</code> 函数也是这个形式，只不过里面有多个求和项(基于当前单词预测窗口内多个单
词)。
</p>

<p>
前面在分析的时候，对概率 \(p(w_t|\cdot)\) 取 <code>log</code> 的原因是:
</p>
<ul class="org-ul">
<li>方便计算，将连乘转换成求和</li>
<li>避免数值计算中的精度丢失或者溢出问题</li>
</ul>

<p>
下面将从信息论的角度来解释这么做的目的。
</p>

<p>
<b><a href="https://baike.baidu.com/item/%E7%86%B5/19190273" target="_blank" rel="noopener">熵</a></b> 用来表示一个系统内在的混乱程度或者不确定性程度，熵越大，混乱程度越高，说明这个系统越不稳定。
反之则说明这个系统越稳定。
</p>

<p>
<b><a href="https://baike.baidu.com/item/%E4%BF%A1%E6%81%AF%E7%86%B5" target="_blank" rel="noopener">信息熵</a></b> 在信息论中，信息熵表示的是信息的不确定性，熵越大，信息越不确定，熵越小， 信息越确定。其
计算公式如下: 
</p>

\begin{equation*}
  H(x) = \sum_{i} q(x_{i}) \cdot log\ \frac{1}{q(x_{i})} = - \sum_{i} q(x_{i}) \cdot log\ q(x_{i})
\end{equation*}

<p>
\(\mathbf{X}\) 表示一个随机变量，\(x_i\) 表示其特定的取值, \(q(x_i)\) 表示对应取值发生的概率。
</p>

<p>
比如,  \(x = 0\) 表示太阳明天不会升起, \(x=1\) 表示太阳明天不会升起，那么，在我们这个世界有 
\(q(x_0) = 0, q(x_1) = 1, H(x) = 0 \cdot log 0 + 1 \cdot log 1\) , 也就是 \(\mathbf{X}\) 这个随机变量的不确定性为0。 
</p>

<p>
从概率分布的角度来说， \(q(x_i)\) 称之为真实分布，很多时候，我们并不知道真实分布, 而是通过另外一个
分布来拟合这个真实分布，这个拟合分布记作 \(p(x_i)\) ，那么，可以通过 <a href="https://baike.baidu.com/item/%E7%9B%B8%E5%AF%B9%E7%86%B5" target="_blank" rel="noopener">KL散度/相对熵</a> 来度量这个拟合的逼近程度:
</p>

\begin{equation*}
  \begin{array}{rcl}
    KL(q,p) & = & \sum_{i} q(x_{i}) \cdot log\ \frac{q(x_{i})}{p(x_{i})} \\
    \\
	   & = & \sum_{i} \left[ q(x_{i}) \cdot log\ q(x_{i})  - q(x_{i}) \cdot log\ p(x_{i}) \right] \\
    \\
	   & = & -H(p) - \sum_{i} q(x_{i}) \cdot log\ p(x_{i})
  \end{array}
\end{equation*}


<p>
KL 散度越小，说明两个分布越接近。上式中第一项是 <code>q</code> 分布的熵， 这一样不会因为 <code>p</code> 改变而改变，而
我们的目标是得到 <code>p</code> 分布，因此第一项可以忽略，优化目标简化如下:
</p>

\begin{equation*}
  \mathop{\arg\min}_{p}\ - \sum_{i}q(x_{i}) \cdot log \ p(x_{i})
\end{equation*}

<p>
其中的 \(q(x_i) \cdot log\ p(x_i)\) 叫做 <a href="https://baike.baidu.com/item/%E4%BA%A4%E5%8F%89%E7%86%B5/8983241" target="_blank" rel="noopener">交叉熵</a> 。 可以看到， <code>loss</code> 函数相对于上面的交叉熵少了一
个求和项，接下来说明两者是等价的。 
</p>

<p>
由上面的分析可以知道，\(q(x_i)\) 是真实概率分布，\(p(x_i)\) 是拟合的概率分布, 对应 <code>skip-gram</code>
中的 \(p(w_j|w_i)\), <code>CBOW</code> 中的 \(p(w_t|h_t)\) , 而真实分布是不知道的, 因为如果知道的话，就不用拟合，
直接拿过来用就是了， 所以只能根据统计学，使用实际观测到的值来模拟, 也就是当前时刻观察到哪个单词，
其概率就是1, 其他的单词出现的概率都是0, 也即 \(p(w_t) = 1,\ p(w_{t^{'}}, t^{'} \neq t) = 0\) , 对
应的分布向量称为 <code>one-hot</code> 向量。由此可得:
</p>

\begin{equation*}
  \begin{array}{rcl} 
    -\sum_{i} q(x_{i}) \cdot log\ p(w_{i}|h_{t}) & = & - 1 \cdot log\ p(w_{t}|h_{t}) - \sum_{i, i\neq t} 0 \cdot log\ p(w_{i}|h_{t}) \\
						 & = & - log\ p(w_{t}|h_{t})
  \end{array}
\end{equation*}

<p>
这是给定一个单词对应的 <code>loss</code> , 如果在整个句子上求平均，正好是 \eqref{eq-cbow-loss} 。所以，\eqref{eq-cbow-loss}
实际上计算的就是交叉熵。
</p>
</div>
</div>

<div id="outline-container-orgf8acf27" class="outline-3">
<h3 id="orgf8acf27">negative sampling</h3>
<div class="outline-text-3" id="text-orgf8acf27">
<p>
上面的分析给出了 <code>softmax loss</code> 函数的本质:
</p>

<pre class="example">
给定一个 one-hot 分布，计算输出的概率分布p和其逼近程度</pre>

<p>
<code>hierarchical softmax</code> 优化没有改变其本质，只不过利用了 <code>one-hot</code> 这个特性，将原本需要 \(V\) 次计
算的 \(p(w_t|\cdot)\) 层次化为 \(log V\) 次计算，如果所有计算所有的 \(p(w_{t^{'}})\) , 那么还是需要 \(V\)
次计算。
</p>

<p>
<code>negative sampling(NEG)</code>, 或者说负采样技术，则另辟蹊径来解决 <code>softmax</code> 的计算问题。其理论依据是
<a class="org-ref-reference" href="#gutmann-2012-noise-contr">gutmann-2012-noise-contr</a> 中提出的 <code>NCE</code> , 后在 <a class="org-ref-reference" href="#mnih-2012-fast-simpl">mnih-2012-fast-simpl</a> 中被用到语言模型的训练中。
细节这里暂且不表，具体做法是:
</p>

<ol class="org-ol">
<li>对于每个目标单词 \(w_t\) , 根据某个特定的分布 \(P_n(w_t)\) , 采样得到一个负例集合 \(C_t = \{c_1, c_2, \cdots, c_K\}, c_i
         \neq w_t\)</li>
<li>训练模型区分 \(\{w_t\} \bigcup C_t\) 这个集合中的元素是否是 \(w_t\)</li>
</ol>

<p>
<a class="org-ref-reference" href="#mnih-2012-fast-simpl">mnih-2012-fast-simpl</a> 中给出的 <code>loss</code> 函数计算比较比较复杂， <code>word2vec</code> 作者在
<a class="org-ref-reference" href="#mikolov-2013-distr-repres">mikolov-2013-distr-repres</a> 中指出，由于 <code>word2vec</code> 只是为了训练得到词的向量表示，并不需要
训练得到语言模型，因此可以简化如下:
</p>

\begin{equation} \label{eq-negative-sampling-loss}
  \begin{array}{rcl}
    \mathcal{L} & = & log\ \sigma(h_{t} \cdot w_{t}) + \sum\limits_{i=1}^{K} \mathbb{E}_{w_{i} \sim P_{n}(w)}\left[ log\ \sigma(-h_{t} \cdot c_{i})\right]
  \end{array}
\end{equation}

<p>
其中，\(P_{n}(w)\) 为 <code>unigram</code> ，并且对其中高频项使用了一个指数惩罚项，也就是 
</p>

\begin{equation*}
  P_{n}(w_{t}) = \frac{1}{Z} \cdot cnt(w_{t})^{3/4}
\end{equation*}

<p>
\(Z\) 为语料中的总的词数，\(cnt(w_t)\) 为 \(w_t\) 在语料中出现的次数。
</p>

<p>
\eqref{eq-negative-sampling-loss} 看起来有点陌生，实际上是老熟人了, 由 <code>sigmod</code> 决策公式 \eqref{eq-sigmoid} 可知
</p>

\begin{equation*}
  \mathcal{L}(x) = l \cdot log\ \sigma(x) + (1 - l) \cdot log\ \left[1 - \sigma(x)\right]
\end{equation*}

<p>
注意到
</p>

\begin{equation*}
  1 - \sigma(x)  \ =\   1 - \frac{1}{1+e^{-x}}  \ =\    \frac{e^{-x}}{1+e^{-x}}  \ =\   \frac{1}{e^{x}+1}  \ =\   \sigma(-x)
\end{equation*}

<p>
所以有
</p>

\begin{equation*}
  \mathcal{L}(x) = l \cdot log\ \sigma(x) + (1 - l) \cdot log\ \sigma(-x)
\end{equation*}

<p>
根据上面 <code>NEG</code> 的做法描述可知，目标单词 \(w_t\) 对应的标签为 \(l = 1\) ，对应的 \(\mathcal{L}(x)\) 中第二项为
零， 负样本 \(c_i\) 对应的标签 \(l_i = 0\) ， 对应的 \(\mathcal{L}(x)\) 中的第一项为0, 因此可以得到:
</p>

\begin{equation*} 
  \begin{array}{rcl}
    \mathcal{L} & = & \mathcal{L}(h_t \cdot w_t) + \sum\limits_{i=1}^{K} \mathbb{E}_{w_{i} \sim P_{n}(w)} \mathcal{L}(h_t \cdot c_i) \\
    \\
		& = & log\ \sigma(h_{t} \cdot w_{t})
		      +
		      \sum\limits_{i=1}^{K} \mathbb{E}_{w_{i} \sim P_{n}(w)} log\ \sigma(- h_{t} \cdot c_{i})
  \end{array}
\end{equation*}

<p>
正好就是 \eqref{eq-negative-sampling-loss} 。 
</p>

<p>
也就是说， <code>NEG</code> 实际上是将一个 \(V\) 分类问题转换成 \(K+1\) 个二分类问题，一般来说， \(K \ll V\) ，从而解决了计算量的问题。
</p>

<p>
关于 <code>NEG</code> , 有下面两点需要注意:
</p>
<ol class="org-ol">
<li>相比于 <code>hierarchical softmax</code>, <code>NEG</code> 可以提高模型的准确率</li>
<li><code>NCE</code> 可以用于语言模型训练，而 <code>NEG</code> 只能用于词向量训练，不可用于语言模型训练</li>
</ol>

<p>
更详细的讨论，可以参考 <a class="org-ref-reference" href="#dyer-2014-notes-noise">dyer-2014-notes-noise</a> , 以及 <code>tensorflow</code> 中 <a href="https://www.tensorflow.org/extras/candidate_sampling.pdf" target="_blank" rel="noopener">candidate sampling</a> 文档,
后面准备单开一篇 blog 来做深入学习。 
</p>
</div>
</div>

<div id="outline-container-org517ee99" class="outline-3">
<h3 id="org517ee99">sub-sampling</h3>
<div class="outline-text-3" id="text-org517ee99">
<p>
<code>sub-sampling</code> 技术, 主要是为了解决前面共现矩阵面临的同样问题，就是语料中某些词语的频率远高于其
它词，在一定程度上会影响词向量的训练。因此在训练过程中，会根据某个词出现的频率高低，按照一定的概
率将其从语料中丢掉, 有点类似于 <code>maked language model</code> 。paper 中给出的对于单词 \(w_i\) ， 其在训
练中被扔掉的概率是
</p>

\begin{equation*}
  p(w_{i}) = 1 - \sqrt{t/f_{w_{i}}}
\end{equation*}

<p>
其中 \(f_{w_i}\) 为 \(w_i\) 在整个语料中出现的概率, \(t\) 为一个给定的阈值，paper中的默认值是 \(1e-5\) ,
\(f_{w_i} < t\) 的单词不会被扔掉。\(f_{w_i}\) 越大，被扔掉的概率越大。
</p>

<p>
代码中实际使用的和 paper 中有些差异:
</p>

\begin{equation*}
  p(w_{i}) = 1 - t/f_{w_{i}} - \sqrt{t/f_{w_{i}}}
\end{equation*}

<p>
也就是 \(f_{w_i} \leq \frac{\sqrt{5}+1}{2} \cdot t\) 的时候不会被扔掉, \(s\) 默认取值是 \(1e-3\) 。 看
起来和 paper 中没什么本质差别，至于为什么使用这个, 原因不明。 
</p>
</div>
</div>

<div id="outline-container-org95483ce" class="outline-3">
<h3 id="org95483ce">phrase</h3>
<div class="outline-text-3" id="text-org95483ce">
<p>
在 <a class="org-ref-reference" href="#mikolov-2013-distr-repres">mikolov-2013-distr-repres</a> 中，作者还提到了一个点，就是有些专有名词由多个单词组成，但
是其意义并非简单的组成它的几个单词的组合，比如， <code>New York</code> , <code>Houston Rockets</code> , 这个时候更合理
的是将这些作为一个单词对待。对此作者提出了一个简单的 <code>data-driven</code> 的方法，就是统计语料中所有
<code>bi-gram</code> , 计算得到一个 <code>score</code> 
</p>

\begin{equation*}
  \mathcal{S}(w_{i}, w_{j}) = \frac{ count(w_{i} w_{j}) - \delta } { count(w_{i}) \times count(w_{j}) }
\end{equation*}

<p>
如果 \(\mathcal{S}(w_i, w_j)\) 大于给定的某个阈值, 那么将 \(w_i, w_j\) 合并为一个单词。重复这个过程 2~4 遍即可。
</p>

<p>
不过这个想法只在 paper 中写到了，源码中并没有看到相关实现。
</p>
</div>
</div>
</div>

<div id="outline-container-org8be7735" class="outline-2">
<h2 id="org8be7735">总结</h2>
</div>
<div id="outline-container-org21e5bc4" class="outline-2">
<h2 id="org21e5bc4">References</h2>
<div class="outline-text-2" id="text-org21e5bc4">
<p>

<ul class="org-ref-bib"><li><a id="pennington-2014-glove">[pennington-2014-glove]</a> <a name="pennington-2014-glove"></a>Jeffrey Pennington, Richard Socher, & Christopher Manning, Glove: Global Vectors for Word Representation, nil, in in: Proceedings of the 2014 Conference on Empirical
                  Methods in Natural Language Processing (EMNLP), edited by (2014)</li>
<li><a id="mikolov-2013-effic-estim">[mikolov-2013-effic-estim]</a> <a name="mikolov-2013-effic-estim"></a>Mikolov, Chen, Corrado, & Dean, Efficient Estimation of Word Representations in  Vector Space, <i>CoRR</i>,  (2013). <a href="http://arxiv.org/abs/1301.3781v3" target="_blank" rel="noopener">link</a>.</li>
<li><a id="mikolov-2013-distr-repres">[mikolov-2013-distr-repres]</a> <a name="mikolov-2013-distr-repres"></a>Mikolov, Sutskever, Chen, , Corrado & Dean, Distributed Representations of Words and Phrases and  Their Compositionality, <i>CoRR</i>,  (2013). <a href="http://arxiv.org/abs/1310.4546v1" target="_blank" rel="noopener">link</a>.</li>
<li><a id="bengio-2003-neural-probab">[bengio-2003-neural-probab]</a> <a name="bengio-2003-neural-probab"></a>Bengio, Ducharme, Vincent, Pascal & Jauvin, A Neural Probabilistic Language Model, <i>Journal of machine learning research</i>, <b>3(Feb)</b>, 1137-1155 (2003). <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="noopener">link</a>.</li>
<li><a id="devlin-2018-bert">[devlin-2018-bert]</a> <a name="devlin-2018-bert"></a>Devlin, Chang, Lee, & Toutanova, Bert: Pre-Training of Deep Bidirectional  Transformers for Language Understanding, <i>CoRR</i>,  (2018). <a href="http://arxiv.org/abs/1810.04805v2" target="_blank" rel="noopener">link</a>.</li>
<li><a id="gutmann-2012-noise-contr">[gutmann-2012-noise-contr]</a> <a name="gutmann-2012-noise-contr"></a>Gutmann & Hyv\"arinen, Noise-Contrastive Estimation of Unnormalized  Statistical Models, With Applications To Natural  Image Statistics, <i>Journal of Machine Learning Research</i>, <b>13(Feb)</b>, 307-361 (2012).</li>
<li><a id="mnih-2012-fast-simpl">[mnih-2012-fast-simpl]</a> <a name="mnih-2012-fast-simpl"></a>Mnih & Teh, A Fast and Simple Algorithm for Training Neural  Probabilistic Language Models, <i>CoRR</i>,  (2012). <a href="http://arxiv.org/abs/1206.6426v1" target="_blank" rel="noopener">link</a>.</li>
<li><a id="dyer-2014-notes-noise">[dyer-2014-notes-noise]</a> <a name="dyer-2014-notes-noise"></a>Dyer, Notes on Noise Contrastive Estimation and Negative  Sampling, <i>CoRR</i>,  (2014). <a href="http://arxiv.org/abs/1410.8251v1" target="_blank" rel="noopener">link</a>.</li>
</ul>

</p>
</div>
</div>
</div>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="hanpu.mwx"
      src="/images/ai.jpeg">
  <p class="site-author-name" itemprop="name">hanpu.mwx</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hanpum" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hanpum" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hanpu.mwx@gmail.com" title="E-Mail → mailto:hanpu.mwx@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hanpu.mwx</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">18k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">17 分钟</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  















    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

    </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
